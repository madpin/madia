{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Source: https://www.pinecone.io/learn/series/langchain/langchain-tools/\n",
    "\"\"\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from math import pi\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"Circumference calculator\"\n",
    "    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius) * 2.0 * pi\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n",
    "\n",
    "\n",
    "class GetClientSpentHistoryTool(BaseTool):\n",
    "    name = \"Client Spent History\"\n",
    "    description = (\n",
    "        \"use this tool to get the historical spent from a client, \"\n",
    "        \"also called advertiser or parent company\"\n",
    "    )\n",
    "\n",
    "    def _run(self, client: str):\n",
    "        return {\n",
    "            \"date\": [\n",
    "                \"2020-jan\",\n",
    "                \"2020-fev\",\n",
    "                \"2020-mar\",\n",
    "                \"2020-apr\",\n",
    "                \"2020-may\",\n",
    "                \"2020-jun\",\n",
    "            ],\n",
    "            \"spent\": [100, 120, 150, 100, 90, 100],\n",
    "        }\n",
    "\n",
    "    def _arun(self, radius: int):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# initialize LLM (we use ChatOpenAI because we'll later define a `chat` agent)\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# initialize conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\", k=5, return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "tools = [CircumferenceTool(), GetClientSpentHistoryTool()]\n",
    "\n",
    "# initialize agent with tools\n",
    "agent = initialize_agent(\n",
    "    agent=\"chat-conversational-react-description\",\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method=\"generate\",\n",
    "    memory=conversational_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Circumference calculator\",\n",
      "    \"action_input\": \"7.81\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m49.071677249072565\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"The circumference of a circle with a radius of 7.81mm is approximately 49.07mm.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'can you calculate the circumference of a circle that has a radius of 7.81mm',\n",
       " 'chat_history': [],\n",
       " 'output': 'The circumference of a circle with a radius of 7.81mm is approximately 49.07mm.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"can you calculate the circumference of a circle that has a radius of 7.81mm\")\n",
    "# Correct Answer: 49.0717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Client Spent History\",\n",
      "    \"action_input\": \"Amazon\"\n",
      "}\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m{'date': ['2020-jan', '2020-fev', '2020-mar', '2020-apr', '2020-may', '2020-jun'], 'spent': [100, 120, 150, 100, 90, 100]}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Amazon spent $100 in January.\"\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How much did Amazon, spent in January?',\n",
       " 'chat_history': [HumanMessage(content='can you calculate the circumference of a circle that has a radius of 7.81mm', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The circumference of a circle with a radius of 7.81mm is approximately 49.07mm.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='How much the client Amazon, spent in January?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='The client Amazon spent $100 in January.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='How much did Amazon, spent in January?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Amazon spent $100 in January.', additional_kwargs={}, example=False)],\n",
       " 'output': 'Amazon spent $100 in January.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"How much did Amazon, spent in January?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| .llm_chain.memory:: >```None```<\n",
      "|| .llm_chain.callbacks:: >```None```<\n",
      "|| .llm_chain.callback_manager:: >```None```<\n",
      "|| .llm_chain.verbose:: >```False```<\n",
      "|| .llm_chain.tags:: >```None```<\n",
      "|| .llm_chain.metadata:: >```None```<\n",
      "|||||| .llm_chain.prompt.input_variables[0]: >```input```<\n",
      "|||||| .llm_chain.prompt.input_variables[1]: >```chat_history```<\n",
      "|||||| .llm_chain.prompt.input_variables[2]: >```agent_scratchpad```<\n",
      "|||| .llm_chain.prompt.output_parser:: >```None```<\n",
      "|||||||| .llm_chain.prompt.messages.('prompt', PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='Assistant is a large language model trained by OpenAI.\\n\\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\\n\\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\\n\\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.', template_format='f-string', validate_template=True))[0]: >```additional_kwargs```<\n",
      "|||||||||| .llm_chain.prompt.messages[0].prompt.output_parser:: >```None```<\n",
      "|||||||||| .llm_chain.prompt.messages[0].prompt.template:: >```Assistant is a large language model trained by OpenAI.\n",
      "\n",
      "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
      "\n",
      "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
      "\n",
      "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.```<\n",
      "|||||||||| .llm_chain.prompt.messages[0].prompt.template_format:: >```f-string```<\n",
      "|||||||||| .llm_chain.prompt.messages[0].prompt.validate_template:: >```True```<\n",
      "|||||||| .llm_chain.prompt.messages[1].variable_name:: >```chat_history```<\n",
      "|||||||||||| .llm_chain.prompt.messages[2].prompt.input_variables[0]: >```input```<\n",
      "|||||||||| .llm_chain.prompt.messages[2].prompt.output_parser:: >```None```<\n",
      "|||||||||| .llm_chain.prompt.messages[2].prompt.template:: >```TOOLS\n",
      "------\n",
      "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n",
      "\n",
      "> Circumference calculator: use this tool when you need to calculate a circumference using the radius of a circle\n",
      "> Client Spent History: use this tool to get the historical spent from a client, also called advertiser or parent company\n",
      "\n",
      "RESPONSE FORMAT INSTRUCTIONS\n",
      "----------------------------\n",
      "\n",
      "When responding to me, please output a response in one of two formats:\n",
      "\n",
      "**Option 1:**\n",
      "Use this if you want the human to use a tool.\n",
      "Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": string, \\ The action to take. Must be one of Circumference calculator, Client Spent History\n",
      "    \"action_input\": string \\ The input to the action\n",
      "}}\n",
      "```\n",
      "\n",
      "**Option #2:**\n",
      "Use this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": string \\ You should put what you want to return to use here\n",
      "}}\n",
      "```\n",
      "\n",
      "USER'S INPUT\n",
      "--------------------\n",
      "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
      "\n",
      "{input}```<\n",
      "|||||||||| .llm_chain.prompt.messages[2].prompt.template_format:: >```f-string```<\n",
      "|||||||||| .llm_chain.prompt.messages[2].prompt.validate_template:: >```True```<\n",
      "|||||||| .llm_chain.prompt.messages[3].variable_name:: >```agent_scratchpad```<\n",
      "|||| .llm_chain.llm.cache:: >```None```<\n",
      "|||| .llm_chain.llm.verbose:: >```False```<\n",
      "|||| .llm_chain.llm.callbacks:: >```None```<\n",
      "|||| .llm_chain.llm.callback_manager:: >```None```<\n",
      "|||| .llm_chain.llm.tags:: >```None```<\n",
      "|||| .llm_chain.llm.metadata:: >```None```<\n",
      "|||| .llm_chain.llm.client:: >```<class 'openai.api_resources.chat_completion.ChatCompletion'>```<\n",
      "|||| .llm_chain.llm.model_name:: >```gpt-3.5-turbo```<\n",
      "|||| .llm_chain.llm.temperature:: >```0.0```<\n",
      "|||| .llm_chain.llm.openai_api_key:: >```Not Today ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~```<\n",
      "|||| .llm_chain.llm.openai_api_base:: >``````<\n",
      "|||| .llm_chain.llm.openai_organization:: >``````<\n",
      "|||| .llm_chain.llm.openai_proxy:: >``````<\n",
      "|||| .llm_chain.llm.request_timeout:: >```None```<\n",
      "|||| .llm_chain.llm.max_retries:: >```6```<\n",
      "|||| .llm_chain.llm.streaming:: >```False```<\n",
      "|||| .llm_chain.llm.n:: >```1```<\n",
      "|||| .llm_chain.llm.max_tokens:: >```None```<\n",
      "|||| .llm_chain.llm.tiktoken_model_name:: >```None```<\n",
      "|| .llm_chain.output_key:: >```text```<\n",
      "|| .llm_chain.return_final_only:: >```True```<\n",
      "|| .allowed_tools[0]: >```Circumference calculator```<\n",
      "|| .allowed_tools[1]: >```Client Spent History```<\n",
      " .template_tool_response:: >```TOOL RESPONSE: \n",
      "---------------------\n",
      "{observation}\n",
      "\n",
      "USER'S INPUT\n",
      "--------------------\n",
      "\n",
      "Okay, so what is the response to my last comment? If using information obtained from the tools you must mention it explicitly without mentioning the tool names - I have forgotten all TOOL RESPONSES! Remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else.```<\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from collections.abc import Iterable\n",
    "\n",
    "pp = pprint.PrettyPrinter(width=41, compact=True)\n",
    "\n",
    "\n",
    "def print_obj(obj, level=0, prev_obj=\"\"):\n",
    "    try:\n",
    "        for x, y in obj:\n",
    "            y = (\n",
    "                y\n",
    "                if x != \"openai_api_key\"\n",
    "                else \"Not Today ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\n",
    "            )\n",
    "            if isinstance(y, Iterable) and not isinstance(y, str):\n",
    "                print_obj(y, level + 1, f\"{prev_obj}.{x}\")\n",
    "            else:\n",
    "                print(f\"{'||'*level} {prev_obj}.{x}:: >```{y}```<\")\n",
    "    except ValueError:\n",
    "        for i, y in enumerate(obj):\n",
    "            if isinstance(y, Iterable) and not isinstance(y, str):\n",
    "                print_obj(y, level + 1, f\"{prev_obj}[{i}]\")\n",
    "            else:\n",
    "                print(f\"{'||'*level} {prev_obj}[{i}]: >```{y}```<\")\n",
    "\n",
    "\n",
    "print_obj(agent.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOLS\n",
      "------\n",
      "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question. The tools the human can use are:\n",
      "\n",
      "> Circumference calculator: use this tool when you need to calculate a circumference using the radius of a circle\n",
      "> Client Spent History: use this tool to get the historical spent from a client, also called advertiser or parent company\n",
      "\n",
      "RESPONSE FORMAT INSTRUCTIONS\n",
      "----------------------------\n",
      "\n",
      "When responding to me, please output a response in one of two formats:\n",
      "\n",
      "**Option 1:**\n",
      "Use this if you want the human to use a tool.\n",
      "Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": string, \\ The action to take. Must be one of Circumference calculator, Client Spent History\n",
      "    \"action_input\": string \\ The input to the action\n",
      "}}\n",
      "```\n",
      "\n",
      "**Option #2:**\n",
      "Use this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n",
      "\n",
      "```json\n",
      "{{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": string \\ You should put what you want to return to use here\n",
      "}}\n",
      "```\n",
      "\n",
      "USER'S INPUT\n",
      "--------------------\n",
      "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
      "\n",
      "{input}\n"
     ]
    }
   ],
   "source": [
    "print(agent.agent.llm_chain.prompt.messages[2].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
